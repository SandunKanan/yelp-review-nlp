{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I3f0Ww0Fh5-ixwxMQ5L1Cg</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A visit to New Orleans is not complete without...</td>\n",
       "      <td>2015-11-06 01:44:28</td>\n",
       "      <td>visit new orleans complete without stop mother...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wgtJ6oS_VIZT_yVz3N5w3Q</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Ferdi was only amazing because of the debr...</td>\n",
       "      <td>2012-12-04 22:34:30</td>\n",
       "      <td>ferdi amazing debris amazing ham roast beef ok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9BzlclzAH_XdTOijPQZaOw</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a regular stop for me when I am in new...</td>\n",
       "      <td>2016-06-12 00:55:19</td>\n",
       "      <td>regular stop new orleans way stop get ferdi sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9jWSprR9OJL4WW1P5iGToQ</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This place gave my fiancé food poisoning today...</td>\n",
       "      <td>2016-01-04 02:48:37</td>\n",
       "      <td>place gave fiancé food poisoning today food re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_KNqLK8xC4xBOCz3Q7MOag</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I tried the spaghetti and meatballs. It was a ...</td>\n",
       "      <td>2018-06-08 22:29:13</td>\n",
       "      <td>tried spaghetti meatballs great dish huge meal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  stars  useful  funny  cool  \\\n",
       "0  I3f0Ww0Fh5-ixwxMQ5L1Cg  iSRTaT9WngzB8JJ2YKJUig    5.0       0      0     0   \n",
       "1  wgtJ6oS_VIZT_yVz3N5w3Q  iSRTaT9WngzB8JJ2YKJUig    4.0       0      0     0   \n",
       "2  9BzlclzAH_XdTOijPQZaOw  iSRTaT9WngzB8JJ2YKJUig    5.0       0      0     0   \n",
       "3  9jWSprR9OJL4WW1P5iGToQ  iSRTaT9WngzB8JJ2YKJUig    1.0       1      0     0   \n",
       "4  _KNqLK8xC4xBOCz3Q7MOag  iSRTaT9WngzB8JJ2YKJUig    5.0       0      0     0   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  A visit to New Orleans is not complete without...  2015-11-06 01:44:28   \n",
       "1  The Ferdi was only amazing because of the debr...  2012-12-04 22:34:30   \n",
       "2  This is a regular stop for me when I am in new...  2016-06-12 00:55:19   \n",
       "3  This place gave my fiancé food poisoning today...  2016-01-04 02:48:37   \n",
       "4  I tried the spaghetti and meatballs. It was a ...  2018-06-08 22:29:13   \n",
       "\n",
       "                                          clean_text  \n",
       "0  visit new orleans complete without stop mother...  \n",
       "1  ferdi amazing debris amazing ham roast beef ok...  \n",
       "2  regular stop new orleans way stop get ferdi sp...  \n",
       "3  place gave fiancé food poisoning today food re...  \n",
       "4  tried spaghetti meatballs great dish huge meal...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mother =pd.read_csv('csv/mother_reviews_clean.csv', index_col=0)\n",
    "mother.reset_index(drop=True, inplace=True)\n",
    "mother.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mother=mother[mother['stars'] == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9jWSprR9OJL4WW1P5iGToQ</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This place gave my fiancé food poisoning today...</td>\n",
       "      <td>2016-01-04 02:48:37</td>\n",
       "      <td>place gave fiancé food poisoning today food re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fq5CvUC1AzK8GSmpyn0UeQ</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Tourist trap.  Go to Cafe Amelie instead - it'...</td>\n",
       "      <td>2015-05-12 18:41:09</td>\n",
       "      <td>tourist trap  go cafe amelie instead   blocks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>l1w-oZxPOVRooai5nwz3Bg</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I was very excited to try this place due to al...</td>\n",
       "      <td>2016-08-04 00:14:41</td>\n",
       "      <td>excited try place due reviews ordered world fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>iXpuRUOYbE4gaogki4qm3Q</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>So sorry that seven of us went here.  The food...</td>\n",
       "      <td>2016-08-12 21:47:54</td>\n",
       "      <td>sorry seven us went  food ridiculously bad \\nw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>FziBgPaiqRJM71FExiJQ3Q</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mother's is not Juicy\\n\\nNothing about this pl...</td>\n",
       "      <td>2011-11-30 06:59:41</td>\n",
       "      <td>mothers juicy\\n\\nnothing place juicy sure fuss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5224</th>\n",
       "      <td>lrLbvgKaA_fmZnPQQMp9zg</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This place came highly recommended and I can't...</td>\n",
       "      <td>2021-12-10 23:57:13</td>\n",
       "      <td>place came highly recommended cant figure remi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5228</th>\n",
       "      <td>0vaaMugPM1uOGDri1T3lJQ</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The food is good, BUT, the service was the wor...</td>\n",
       "      <td>2021-07-31 18:17:45</td>\n",
       "      <td>food good service worst ive life  expected hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5230</th>\n",
       "      <td>YXKSQvFAuoMTwEZXQD97Ow</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The staff was rude and could not comprehend a ...</td>\n",
       "      <td>2018-01-10 13:57:52</td>\n",
       "      <td>staff rude could comprehend simple order trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5240</th>\n",
       "      <td>n2SX-tKhr5AkjmG30-osLQ</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(re rated @ ONE after zero contact, I tried, i...</td>\n",
       "      <td>2019-04-18 17:37:27</td>\n",
       "      <td>rated  one zero contact tried appears havent\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5251</th>\n",
       "      <td>IoZfBwHCO244npWM6EVhZA</td>\n",
       "      <td>iSRTaT9WngzB8JJ2YKJUig</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Total tourist trap. Avoid. Prices are high; fo...</td>\n",
       "      <td>2018-03-12 15:32:26</td>\n",
       "      <td>total tourist trap avoid prices high food accu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>659 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user_id             business_id  stars  useful  funny  \\\n",
       "3     9jWSprR9OJL4WW1P5iGToQ  iSRTaT9WngzB8JJ2YKJUig    1.0       1      0   \n",
       "9     Fq5CvUC1AzK8GSmpyn0UeQ  iSRTaT9WngzB8JJ2YKJUig    1.0       0      0   \n",
       "35    l1w-oZxPOVRooai5nwz3Bg  iSRTaT9WngzB8JJ2YKJUig    1.0       0      0   \n",
       "39    iXpuRUOYbE4gaogki4qm3Q  iSRTaT9WngzB8JJ2YKJUig    1.0       0      0   \n",
       "43    FziBgPaiqRJM71FExiJQ3Q  iSRTaT9WngzB8JJ2YKJUig    1.0       3      0   \n",
       "...                      ...                     ...    ...     ...    ...   \n",
       "5224  lrLbvgKaA_fmZnPQQMp9zg  iSRTaT9WngzB8JJ2YKJUig    1.0       1      0   \n",
       "5228  0vaaMugPM1uOGDri1T3lJQ  iSRTaT9WngzB8JJ2YKJUig    1.0       2      0   \n",
       "5230  YXKSQvFAuoMTwEZXQD97Ow  iSRTaT9WngzB8JJ2YKJUig    1.0       1      0   \n",
       "5240  n2SX-tKhr5AkjmG30-osLQ  iSRTaT9WngzB8JJ2YKJUig    1.0       3      0   \n",
       "5251  IoZfBwHCO244npWM6EVhZA  iSRTaT9WngzB8JJ2YKJUig    1.0       2      0   \n",
       "\n",
       "      cool                                               text  \\\n",
       "3        0  This place gave my fiancé food poisoning today...   \n",
       "9        0  Tourist trap.  Go to Cafe Amelie instead - it'...   \n",
       "35       0  I was very excited to try this place due to al...   \n",
       "39       0  So sorry that seven of us went here.  The food...   \n",
       "43       0  Mother's is not Juicy\\n\\nNothing about this pl...   \n",
       "...    ...                                                ...   \n",
       "5224     0  This place came highly recommended and I can't...   \n",
       "5228     0  The food is good, BUT, the service was the wor...   \n",
       "5230     0  The staff was rude and could not comprehend a ...   \n",
       "5240     0  (re rated @ ONE after zero contact, I tried, i...   \n",
       "5251     0  Total tourist trap. Avoid. Prices are high; fo...   \n",
       "\n",
       "                     date                                         clean_text  \n",
       "3     2016-01-04 02:48:37  place gave fiancé food poisoning today food re...  \n",
       "9     2015-05-12 18:41:09  tourist trap  go cafe amelie instead   blocks ...  \n",
       "35    2016-08-04 00:14:41  excited try place due reviews ordered world fa...  \n",
       "39    2016-08-12 21:47:54  sorry seven us went  food ridiculously bad \\nw...  \n",
       "43    2011-11-30 06:59:41  mothers juicy\\n\\nnothing place juicy sure fuss...  \n",
       "...                   ...                                                ...  \n",
       "5224  2021-12-10 23:57:13  place came highly recommended cant figure remi...  \n",
       "5228  2021-07-31 18:17:45  food good service worst ive life  expected hou...  \n",
       "5230  2018-01-10 13:57:52  staff rude could comprehend simple order trans...  \n",
       "5240  2019-04-18 17:37:27  rated  one zero contact tried appears havent\\n...  \n",
       "5251  2018-03-12 15:32:26  total tourist trap avoid prices high food accu...  \n",
       "\n",
       "[659 rows x 9 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absolutely disgusting</th>\n",
       "      <th>absolutely horrible</th>\n",
       "      <th>accept</th>\n",
       "      <th>accept tips</th>\n",
       "      <th>ache</th>\n",
       "      <th>across</th>\n",
       "      <th>across street</th>\n",
       "      <th>acted</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>young lady</th>\n",
       "      <th>yuck</th>\n",
       "      <th>zero</th>\n",
       "      <th>zero stars</th>\n",
       "      <th>étouffée</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.175917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>659 rows × 1976 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     able  absolutely  absolutely disgusting  absolutely horrible  accept  \\\n",
       "0     0.0         0.0                    0.0                  0.0     0.0   \n",
       "1     0.0         0.0                    0.0                  0.0     0.0   \n",
       "2     0.0         0.0                    0.0                  0.0     0.0   \n",
       "3     0.0         0.0                    0.0                  0.0     0.0   \n",
       "4     0.0         0.0                    0.0                  0.0     0.0   \n",
       "..    ...         ...                    ...                  ...     ...   \n",
       "654   0.0         0.0                    0.0                  0.0     0.0   \n",
       "655   0.0         0.0                    0.0                  0.0     0.0   \n",
       "656   0.0         0.0                    0.0                  0.0     0.0   \n",
       "657   0.0         0.0                    0.0                  0.0     0.0   \n",
       "658   0.0         0.0                    0.0                  0.0     0.0   \n",
       "\n",
       "     accept tips  ache  across  across street  acted  ...  yes  yesterday  \\\n",
       "0            0.0   0.0     0.0            0.0    0.0  ...  0.0        0.0   \n",
       "1            0.0   0.0     0.0            0.0    0.0  ...  0.0        0.0   \n",
       "2            0.0   0.0     0.0            0.0    0.0  ...  0.0        0.0   \n",
       "3            0.0   0.0     0.0            0.0    0.0  ...  0.0        0.0   \n",
       "4            0.0   0.0     0.0            0.0    0.0  ...  0.0        0.0   \n",
       "..           ...   ...     ...            ...    ...  ...  ...        ...   \n",
       "654          0.0   0.0     0.0            0.0    0.0  ...  0.0        0.0   \n",
       "655          0.0   0.0     0.0            0.0    0.0  ...  0.0        0.0   \n",
       "656          0.0   0.0     0.0            0.0    0.0  ...  0.0        0.0   \n",
       "657          0.0   0.0     0.0            0.0    0.0  ...  0.0        0.0   \n",
       "658          0.0   0.0     0.0            0.0    0.0  ...  0.0        0.0   \n",
       "\n",
       "     yet  you  young  young lady  yuck      zero  zero stars  étouffée  \n",
       "0    0.0  0.0    0.0         0.0   0.0  0.000000         0.0  0.000000  \n",
       "1    0.0  0.0    0.0         0.0   0.0  0.000000         0.0  0.000000  \n",
       "2    0.0  0.0    0.0         0.0   0.0  0.000000         0.0  0.000000  \n",
       "3    0.0  0.0    0.0         0.0   0.0  0.000000         0.0  0.175917  \n",
       "4    0.0  0.0    0.0         0.0   0.0  0.000000         0.0  0.000000  \n",
       "..   ...  ...    ...         ...   ...       ...         ...       ...  \n",
       "654  0.0  0.0    0.0         0.0   0.0  0.000000         0.0  0.000000  \n",
       "655  0.0  0.0    0.0         0.0   0.0  0.000000         0.0  0.000000  \n",
       "656  0.0  0.0    0.0         0.0   0.0  0.129597         0.0  0.000000  \n",
       "657  0.0  0.0    0.0         0.0   0.0  0.068868         0.0  0.000000  \n",
       "658  0.0  0.0    0.0         0.0   0.0  0.000000         0.0  0.000000  \n",
       "\n",
       "[659 rows x 1976 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), min_df = 0.005)\n",
    "\n",
    "vectorized_text = vectorizer.fit_transform(mother['clean_text'])\n",
    "vectorized_text = pd.DataFrame(\n",
    "    vectorized_text.toarray(),\n",
    "    columns = vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(max_iter=500, n_components=50, n_jobs=-1,\n",
       "                          random_state=1, topic_word_prior=0.5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(max_iter=500, n_components=50, n_jobs=-1,\n",
       "                          random_state=1, topic_word_prior=0.5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(max_iter=500, n_components=50, n_jobs=-1,\n",
       "                          random_state=1, topic_word_prior=0.5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Instantiate the LDA\n",
    "n_components = 50\n",
    "lda_model = LatentDirichletAllocation(n_components=n_components,\n",
    "                                      max_iter = 500,\n",
    "                                      random_state=1,\n",
    "                                      n_jobs = -1,\n",
    "                                      topic_word_prior=0.5)\n",
    "\n",
    "# Fit the LDA on the vectorized documents\n",
    "lda_model.fit(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(659, 50)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_mixture = lda_model.transform(vectorized_text)\n",
    "document_mixture.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_mixture = pd.DataFrame(\n",
    "    lda_model.components_,\n",
    "    columns = vectorizer.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(model, vectorizer):\n",
    "    topics = []\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        topics.append([(vectorizer.get_feature_names_out()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-10 - 1:-1]])\n",
    "    return topics\n",
    "\n",
    "topics = get_topics(lda_model, vectorizer)\n",
    "topic_words = [[word[0] for word in topic] for topic in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['étouffée',\n",
       " 'god',\n",
       " 'going',\n",
       " 'going back',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'good bread',\n",
       " 'good experience',\n",
       " 'good food',\n",
       " 'good fried']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_complaints =pd.read_csv('csv/mother_complaints.csv', index_col=0)\n",
    "top_300 = mother_complaints.iloc[:300].reset_index().rename(columns={'index':'phrase'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>place dirty</td>\n",
       "      <td>1.236985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rude service</td>\n",
       "      <td>1.165142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filthy</td>\n",
       "      <td>1.114094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disgusting</td>\n",
       "      <td>0.901551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worse</td>\n",
       "      <td>0.869195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>large</td>\n",
       "      <td>0.188984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>seafood gumbo</td>\n",
       "      <td>0.188438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>seafood</td>\n",
       "      <td>0.188052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>wanted</td>\n",
       "      <td>0.187908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>tell</td>\n",
       "      <td>0.186907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            phrase     score\n",
       "0      place dirty  1.236985\n",
       "1     rude service  1.165142\n",
       "2           filthy  1.114094\n",
       "3       disgusting  0.901551\n",
       "4            worse  0.869195\n",
       "..             ...       ...\n",
       "295          large  0.188984\n",
       "296  seafood gumbo  0.188438\n",
       "297        seafood  0.188052\n",
       "298         wanted  0.187908\n",
       "299           tell  0.186907\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_allocations = topic_word_mixture.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolutely</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolutely disgusting</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolutely horrible</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accept</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0    1    2    3    4    5    6    7    8    9   ...  \\\n",
       "able                   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  ...   \n",
       "absolutely             0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  ...   \n",
       "absolutely disgusting  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  ...   \n",
       "absolutely horrible    0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  ...   \n",
       "accept                 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  ...   \n",
       "\n",
       "                        40   41   42   43   44   45   46   47   48   49  \n",
       "able                   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "absolutely             0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "absolutely disgusting  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "absolutely horrible    0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "accept                 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_allocations.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Topic     Score\n",
      "able                      15  1.923979\n",
      "absolutely                15  4.469360\n",
      "absolutely disgusting     15  1.226454\n",
      "absolutely horrible       15  1.368925\n",
      "accept                    15  1.002737\n",
      "...                      ...       ...\n",
      "young lady                15  1.610270\n",
      "yuck                      15  1.966779\n",
      "zero                      15  2.994577\n",
      "zero stars                15  1.369597\n",
      "étouffée                  15  1.871616\n",
      "\n",
      "[1976 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'word_allocations' is your original DataFrame with word distributions across topics\n",
    "# You can replace the data and index in word_allocations with your actual data\n",
    "\n",
    "# Find the column (topic) with the highest score for each word\n",
    "max_topic = word_allocations.idxmax(axis=1)\n",
    "\n",
    "# Extract the highest score for each word\n",
    "max_score = word_allocations.max(axis=1)\n",
    "\n",
    "# Create a new DataFrame with the word as the index, and two columns: one for the topic with the highest score and one for the score itself\n",
    "new_df = pd.DataFrame({\n",
    "    'Topic': max_topic,\n",
    "    'Score': max_score\n",
    "})\n",
    "\n",
    "# If you want to see the DataFrame\n",
    "print(new_df)\n",
    "\n",
    "# If you need to use this DataFrame for further analysis or save it, you can do so directly.\n",
    "# For example, to save this DataFrame to a CSV file:\n",
    "# new_df.to_csv('path_to_save/word_topic_distribution.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'phrase'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/core/indexes/base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'phrase'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming 'word_allocations' is your DataFrame with the word distributions\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# And 'top_100' is another DataFrame with your words stored in the column \"phrase\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Filter the original DataFrame to keep only the rows for words in top_100\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m word_allocations\u001b[38;5;241m.\u001b[39mloc[word_allocations\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39misin(\u001b[43mtop_300\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mphrase\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Find the column (topic) with the highest score for each word in the filtered DataFrame\u001b[39;00m\n\u001b[1;32m     10\u001b[0m max_topic \u001b[38;5;241m=\u001b[39m filtered_df\u001b[38;5;241m.\u001b[39midxmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/core/indexes/base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3631\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3632\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3633\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3634\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'phrase'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'word_allocations' is your DataFrame with the word distributions\n",
    "# And 'top_100' is another DataFrame with your words stored in the column \"phrase\"\n",
    "\n",
    "# Filter the original DataFrame to keep only the rows for words in top_100\n",
    "filtered_df = word_allocations.loc[word_allocations.index.isin(top_300['phrase'])]\n",
    "\n",
    "# Find the column (topic) with the highest score for each word in the filtered DataFrame\n",
    "max_topic = filtered_df.idxmax(axis=1)\n",
    "\n",
    "# Extract the highest score for each word in the filtered DataFrame\n",
    "max_score = filtered_df.max(axis=1)\n",
    "\n",
    "# Create a new DataFrame from the filtered words with the topic and score\n",
    "new_df = pd.DataFrame({\n",
    "    'Topic': max_topic,\n",
    "    'topic_likelihood': max_score\n",
    "})\n",
    "\n",
    "# If you want to see the DataFrame\n",
    "new_df\n",
    "\n",
    "# Optional: Saving the new DataFrame to a CSV file\n",
    "# new_df.to_csv('path_to_save/top_100_word_topic_distribution.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['phrase'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtop_300\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mphrase\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Remove the name of the index\u001b[39;00m\n\u001b[1;32m      3\u001b[0m top_300\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/core/frame.py:5503\u001b[0m, in \u001b[0;36mDataFrame.set_index\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   5500\u001b[0m                 missing\u001b[38;5;241m.\u001b[39mappend(col)\n\u001b[1;32m   5502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m-> 5503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are in the columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   5506\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['phrase'] are in the columns\""
     ]
    }
   ],
   "source": [
    "top_300.set_index('phrase', inplace=True)\n",
    "# Remove the name of the index\n",
    "top_300.index.name = None\n",
    "\n",
    "# Now, top_100's index (which contains what were previously the values of \"phrase\") will have no name.\n",
    "\n",
    "top_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>topic_likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>absolutely</th>\n",
       "      <td>12</td>\n",
       "      <td>0.745416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actually</th>\n",
       "      <td>12</td>\n",
       "      <td>0.667489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>almost</th>\n",
       "      <td>24</td>\n",
       "      <td>0.726673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>already</th>\n",
       "      <td>27</td>\n",
       "      <td>0.755187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anyone</th>\n",
       "      <td>46</td>\n",
       "      <td>0.706237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worse</th>\n",
       "      <td>28</td>\n",
       "      <td>0.726248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>42</td>\n",
       "      <td>0.742516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrong</th>\n",
       "      <td>36</td>\n",
       "      <td>0.698589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>years</th>\n",
       "      <td>12</td>\n",
       "      <td>0.699267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>16</td>\n",
       "      <td>0.613953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Topic  topic_likelihood\n",
       "absolutely     12          0.745416\n",
       "actually       12          0.667489\n",
       "almost         24          0.726673\n",
       "already        27          0.755187\n",
       "anyone         46          0.706237\n",
       "...           ...               ...\n",
       "worse          28          0.726248\n",
       "worst          42          0.742516\n",
       "wrong          36          0.698589\n",
       "years          12          0.699267\n",
       "zero           16          0.613953\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>Topic</th>\n",
       "      <th>topic_likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>place dirty</th>\n",
       "      <td>1.236985</td>\n",
       "      <td>26</td>\n",
       "      <td>0.699292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rude service</th>\n",
       "      <td>1.165142</td>\n",
       "      <td>46</td>\n",
       "      <td>0.788919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filthy</th>\n",
       "      <td>1.114094</td>\n",
       "      <td>20</td>\n",
       "      <td>0.659571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgusting</th>\n",
       "      <td>0.901551</td>\n",
       "      <td>21</td>\n",
       "      <td>0.734964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worse</th>\n",
       "      <td>0.869195</td>\n",
       "      <td>28</td>\n",
       "      <td>0.726248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>large</th>\n",
       "      <td>0.188984</td>\n",
       "      <td>16</td>\n",
       "      <td>0.784966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seafood gumbo</th>\n",
       "      <td>0.188438</td>\n",
       "      <td>7</td>\n",
       "      <td>0.708587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seafood</th>\n",
       "      <td>0.188052</td>\n",
       "      <td>7</td>\n",
       "      <td>0.917308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wanted</th>\n",
       "      <td>0.187908</td>\n",
       "      <td>37</td>\n",
       "      <td>0.701302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tell</th>\n",
       "      <td>0.186907</td>\n",
       "      <td>39</td>\n",
       "      <td>0.665940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  score  Topic  topic_likelihood\n",
       "place dirty    1.236985     26          0.699292\n",
       "rude service   1.165142     46          0.788919\n",
       "filthy         1.114094     20          0.659571\n",
       "disgusting     0.901551     21          0.734964\n",
       "worse          0.869195     28          0.726248\n",
       "...                 ...    ...               ...\n",
       "large          0.188984     16          0.784966\n",
       "seafood gumbo  0.188438      7          0.708587\n",
       "seafood        0.188052      7          0.917308\n",
       "wanted         0.187908     37          0.701302\n",
       "tell           0.186907     39          0.665940\n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihoods = top_300.join(new_df)\n",
    "\n",
    "likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>Topic</th>\n",
       "      <th>topic_likelihood</th>\n",
       "      <th>scored_likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>place dirty</th>\n",
       "      <td>1.236985</td>\n",
       "      <td>26</td>\n",
       "      <td>0.699292</td>\n",
       "      <td>0.865014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rude service</th>\n",
       "      <td>1.165142</td>\n",
       "      <td>46</td>\n",
       "      <td>0.788919</td>\n",
       "      <td>0.919203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filthy</th>\n",
       "      <td>1.114094</td>\n",
       "      <td>20</td>\n",
       "      <td>0.659571</td>\n",
       "      <td>0.734824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgusting</th>\n",
       "      <td>0.901551</td>\n",
       "      <td>21</td>\n",
       "      <td>0.734964</td>\n",
       "      <td>0.662607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worse</th>\n",
       "      <td>0.869195</td>\n",
       "      <td>28</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.631251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>large</th>\n",
       "      <td>0.188984</td>\n",
       "      <td>16</td>\n",
       "      <td>0.784966</td>\n",
       "      <td>0.148346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seafood gumbo</th>\n",
       "      <td>0.188438</td>\n",
       "      <td>7</td>\n",
       "      <td>0.708587</td>\n",
       "      <td>0.133525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seafood</th>\n",
       "      <td>0.188052</td>\n",
       "      <td>7</td>\n",
       "      <td>0.917308</td>\n",
       "      <td>0.172501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wanted</th>\n",
       "      <td>0.187908</td>\n",
       "      <td>37</td>\n",
       "      <td>0.701302</td>\n",
       "      <td>0.131780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tell</th>\n",
       "      <td>0.186907</td>\n",
       "      <td>39</td>\n",
       "      <td>0.665940</td>\n",
       "      <td>0.124469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  score  Topic  topic_likelihood  scored_likelihood\n",
       "place dirty    1.236985     26          0.699292           0.865014\n",
       "rude service   1.165142     46          0.788919           0.919203\n",
       "filthy         1.114094     20          0.659571           0.734824\n",
       "disgusting     0.901551     21          0.734964           0.662607\n",
       "worse          0.869195     28          0.726248           0.631251\n",
       "...                 ...    ...               ...                ...\n",
       "large          0.188984     16          0.784966           0.148346\n",
       "seafood gumbo  0.188438      7          0.708587           0.133525\n",
       "seafood        0.188052      7          0.917308           0.172501\n",
       "wanted         0.187908     37          0.701302           0.131780\n",
       "tell           0.186907     39          0.665940           0.124469\n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihoods['scored_likelihood'] = likelihoods['topic_likelihood'] * likelihoods['score']\n",
    "likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic\n",
       "7     15.401252\n",
       "44     5.730996\n",
       "46     4.528745\n",
       "12     4.038086\n",
       "20     4.001146\n",
       "32     3.946761\n",
       "36     3.196664\n",
       "16     2.302503\n",
       "24     2.267615\n",
       "42     2.056387\n",
       "14     1.870754\n",
       "21     1.800752\n",
       "28     1.761497\n",
       "39     1.643050\n",
       "9      1.516971\n",
       "27     1.485047\n",
       "0      1.378300\n",
       "1      1.346380\n",
       "33     1.130997\n",
       "37     1.100387\n",
       "26     1.074924\n",
       "40     0.978070\n",
       "10     0.976083\n",
       "4      0.967754\n",
       "23     0.931177\n",
       "3      0.886130\n",
       "19     0.864802\n",
       "47     0.839187\n",
       "30     0.833197\n",
       "15     0.803317\n",
       "49     0.554338\n",
       "34     0.524707\n",
       "18     0.518248\n",
       "17     0.429814\n",
       "45     0.410547\n",
       "41     0.385938\n",
       "2      0.235436\n",
       "48     0.187189\n",
       "29     0.152810\n",
       "Name: scored_likelihood, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_topic_score = likelihoods.groupby('Topic')['scored_likelihood'].sum()\n",
    "total_topic_score = total_topic_score.sort_values(ascending=False)\n",
    "total_topic_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 7, 44, 46, 12, 20, 32, 36, 16, 24, 42, 14, 21, 28, 39,  9, 27,  0,  1,\n",
       "       33, 37, 26, 40, 10,  4, 23,  3, 19, 47, 30, 15, 49, 34, 18, 17, 45, 41,\n",
       "        2, 48, 29],\n",
       "      dtype='int64', name='Topic')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topics = total_topic_score.index\n",
    "top_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['food cold', 'waste', 'food bland', 'mother', 'mediocre', 'money', 'excited', 'bland', 'live', 'mess', 'workers', 'customers', 'literally', 'standing', 'employees', 'butter', 'hype', 'cashier', 'sitting', 'saying', 'extremely', 'serving', 'customer', 'understand', 'away', 'gave', 'left', 'sausage', 'cabbage', 'receipt', 'finally', 'water', 'reviews', 'rice', 'disappointed', 'else', 'later', 'told', 'drinks', 'everyone', 'anything', 'whole', 'seafood', 'wall', 'since', 'someone', 'something', 'comes', 'po', 'small', 'poboy', 'eating', 'thing', 'either', 'trying', 'tip', 'beans', 'far', 'waiting', 'served', 'many', 'visiting', 'arrived', 'seafood gumbo', 'shrimp poboy'], ['terrible', 'cost', 'dirty', 'used', 'plenty', 'completely', 'establishment', 'tasted', 'extra', 'clean', 'seated', 'tasted like', 'platter', 'asked', 'mothers', 'boys', 'rest', 'old', 'top', 'front', 'walked', 'reason', 'oh', 'tables'], ['rude service', 'save', 'disappointment', 'avoid', 'tourists', 'cold', 'anyone', 'sat', 'kitchen', 'restaurants', 'fries', 'hungry', 'ask', 'husband', 'greens'], ['rave', 'overrated', 'overpriced', 'please', 'register', 'slow', 'absolutely', 'flavor', 'given', 'walk', 'sit', 'man', 'door', 'shrimp po', 'ordering', 'years', 'quite', 'actually'], ['filthy', 'horrible', 'believe', 'seasoned', 'white', 'guy', 'black', 'manager', 'star', 'plate', 'decided', 'city', 'southern', 'real'], ['stay away', 'gotten', 'skip', 'tourist', 'taking', 'behind', 'expected', 'counter', 'says', 'stay', 'business', 'red', 'average', 'getting', 'felt', 'cooked'], ['famous ham', 'look', 'ham', 'wrong', 'point', 'famous', 'least', 'line door', 'ate', 'wife', 'dishes', 'heard', 'ferdi', 'decent', 'pm', 'fish', 'three'], ['waste money', 'zero', 'never go', 'french', 'needed', 'taste', 'large', 'need', 'maybe'], ['stale', 'needs', 'popular', 'party', 'disappointing', 'almost', 'waiter', 'tried', 'thats'], ['awful', 'worst', 'elsewhere', 'soggy', 'lady', 'coffee', 'work']]\n"
     ]
    }
   ],
   "source": [
    "topic_words = []\n",
    "for i in range(10):\n",
    "    new_list= list(likelihoods[likelihoods['Topic'] == top_topics[i]].sort_values(by='scored_likelihood', ascending=False).index)\n",
    "    topic_words.append(new_list)\n",
    "\n",
    "print(topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
